---
title: "AirBnB"
output: html_document
date: "2023-04-16"
---

```{r}
knitr::opts_chunk$set(echo = TRUE)
library(mice)
library(dplyr)
library(tidyr)
library(tidytext)
library(stringr)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(corrplot)
library(MASS)
library(glmnet)
library(caret)
library(Metrics)
library(plotly)
```

#Load Data

```{r}
data_dir <- "../data/"
listing <- read.csv(paste0(data_dir,"listings.csv"))
listing2 <- read.csv(paste0(data_dir, "listings 2.csv"))
neighbourhoods <- read.csv(paste0(data_dir, "neighbourhoods.csv"))
reviews <- read.csv(paste0(data_dir, "reviews.csv"))
reviews2 <- read.csv(paste0(data_dir, "reviews 2.csv"))

# The data covers the quarterly data from 2022
```

#Data Preparation
```{r}
# Select relevant features
listing_col <- c("id", "neighbourhood_group", "neighbourhood", "latitude", "longitude", "room_type", "price")
listing2_col <- c("id", "property_type", "accommodates", "bedrooms", "beds", "amenities", "review_scores_rating",
                  "review_scores_accuracy", "review_scores_cleanliness", "review_scores_checkin", "review_scores_communication", 
                  "review_scores_location", "review_scores_value", "instant_bookable")
listing_sub <- subset(listing, select=listing_col)
listing2_sub <- subset(listing2,select=listing2_col)

# Merge listing and listing2
data <- merge(listing_sub, listing2_sub, by = "id", all.x = TRUE)

# Check for missing values
print("Missing values:")
colSums(is.na(data))
```

#Impute missing values using MICE imputation
```{r}
# Set the filename for the imputed data file
filename <- paste0(data_dir,"imputed_df.csv")

# Check if the imputed data file exists in the directory
if (file.exists(filename)) {
  # If the file exists, read it into a data frame
  data <- read.csv(filename)
} else {
  # If the file does not exist, perform MICE imputation to generate the imputed data frame
  # Select variables with missing values
  vars <- c("bedrooms", "beds", "review_scores_rating", "review_scores_accuracy", 
            "review_scores_cleanliness", "review_scores_checkin", "review_scores_communication", 
            "review_scores_location", "review_scores_value")
  
  # Perform MICE imputation
  imp <- mice(data[, vars], method = "pmm", m = 5, maxit = 50, seed = 12)
  
  # Replace the missing values in the original dataset with the imputed values
  data[, vars] <- complete(imp)
  
  # Check for any remaining missing values
  sapply(data, function(x) sum(is.na(x)))
  
  # Save the imputed data frame to a CSV file
  write.csv(data, paste0(data_dir,"imputed_df.csv"), row.names = FALSE)
}

# Print a summary of the imputed data frame
summary(data)

```

#Exploratory Data Analysis (EDA)
```{r}
# Summarize the numerical variables
summary(data$price)
summary(data$review_scores_rating)

# Plot the distribution of the price variable
bin_width = 50
price_distribution_plot <- ggplot(data, aes(price)) + 
  geom_histogram(aes(y = ..density.. * bin_width), binwidth = bin_width, fill = "lightblue", color = "steelblue") + 
  geom_density(aes(y = ..density.. * bin_width), alpha = 0.5, color = "darkblue") +
  xlim(0, 3000) +
  labs(title = "Price Distribution", y = "Density")
ggsave(filename = "../out/price_distribution_plot.png", plot = price_distribution_plot)

# Plot the relationship between price and number of bedrooms
box_plot <- ggplot(data, aes(x = factor(bedrooms), y = price, fill = factor(bedrooms))) + 
  geom_boxplot() +
  labs(title = "Price vs. Bedrooms", x = "Bedrooms", y = "Price", fill = "Bedrooms")
ggsave(filename = "../out/price_vs_bedrooms_boxplot.png", plot = box_plot)


# Show the average price for the top 20 most popular property types and sort the plot
top_property_types <- data %>% 
  group_by(property_type) %>% 
  summarize(num_properties = n()) %>% 
  top_n(20, num_properties)

avg_price_plot <- data %>% 
  filter(property_type %in% top_property_types$property_type) %>% 
  group_by(property_type) %>% 
  summarize(avg_price = mean(price)) %>% 
  arrange(avg_price) %>% 
  mutate(property_type_ordered = factor(property_type, levels = property_type)) %>% 
  ggplot(aes(x = property_type_ordered, y = avg_price, fill = property_type_ordered)) +
  geom_bar(stat = "identity") +
  scale_fill_viridis_d() +
  coord_flip() +
  labs(x = "Property Type", y = "Average Price", title = "Average Price by Property Type") +
  theme(legend.position = "none")

ggsave(filename = "../out/average_price_by_property_type_no_legend.png", plot = avg_price_plot)

# Get the top 20 neighborhoods by average price
top_neighborhoods <- data %>% 
  group_by(neighbourhood) %>% 
  summarize(avg_price = mean(price)) %>% 
  top_n(20, avg_price)

avg_price_neighborhood_plot <- data %>% 
  filter(neighbourhood %in% top_neighborhoods$neighbourhood) %>% 
  group_by(neighbourhood) %>% 
  summarize(avg_price = mean(price)) %>% 
  arrange(avg_price) %>% 
  mutate(neighbourhood_ordered = factor(neighbourhood, levels = neighbourhood)) %>% 
  ggplot(aes(x = neighbourhood_ordered, y = avg_price, fill = neighbourhood_ordered)) +
  geom_col() +
  scale_fill_viridis_d() +
  coord_flip() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8)) +
  theme(plot.margin = unit(c(1, 1, 1, 1), "cm")) +
  labs(x = "Neighborhood", y = "Average Price", title = "Average Price by Neighborhood") +
  theme(legend.position = "none")

ggsave(filename = "../out/average_price_by_neighborhood_no_legend.png", plot = avg_price_neighborhood_plot)

# Create a new variable for the month of the year
#data$month <- month(ymd(data$last_review))

# Plot the number of reviews by month
#data %>% 
 # group_by(month) %>% 
#  summarize(num_reviews = n()) %>% 
 # ggplot(aes(x = month, y = num_reviews)) +
 # geom_line() +
 # scale_x_continuous(breaks = 1:12, labels = month.abb)
```

#Data Preprocessing
```{r}
# Function to clean the column names
clean_colnames <- function(x) {
  x <- tolower(x)
  x <- str_remove_all(x, "[[:punct:]]")
  x
}

# Read the CSV file
df <- read.csv(paste0(data_dir,"imputed_df.csv"))
               
# Convert instant_bookable to a binary numeric column
df$instant_bookable <- ifelse(df$instant_bookable == "t", 1, 0)

# Create dummy variables for categorical columns
cat_columns <- c("neighbourhood_group", "neighbourhood", "room_type", "property_type")

for (col_name in cat_columns) {
  dummy_matrix <- model.matrix(~ 0 + ., data = df[col_name])
  colnames(dummy_matrix) <- gsub("^[^:]+:([^:]+)$", paste0(col_name, "_"), colnames(dummy_matrix))
  dummy_df <- as.data.frame(dummy_matrix)
  df <- cbind(df, dummy_df)
}

# Remove original categorical columns
df <- df[, !(colnames(df) %in% cat_columns)]

# Clean amenities and create binary columns for each amenity
df <- df %>%
  mutate(amenities = str_remove_all(amenities, "\\[|\\]|\"")) %>%
  separate_rows(amenities, sep = ", ") %>%
  mutate(amenity_clean = clean_colnames(amenities)) %>%
  mutate(amenities = paste0("amenity_", amenity_clean),
         amenities = ifelse(substr(amenities, 1, 10) == "amenity__", substr(amenities, 10), amenities),
         value = 1) %>%
  distinct(id, amenities, .keep_all = TRUE)

# Remove the 'amenity_clean' column
df <- dplyr::select(df, -amenity_clean)

## Keep only the top 100 amenities and additional amenities
amenities_wide_df <- df %>% pivot_wider(names_from = amenities, values_from = value, values_fill = 0)
amenity_columns <- grep("^amenity_", colnames(amenities_wide_df), value = TRUE)
amenity_sums <- colSums(amenities_wide_df[amenity_columns], na.rm = TRUE)
sorted_amenities <- sort(amenity_sums, decreasing = TRUE)
top_100_amenities <- names(sorted_amenities)[1:100]
additional_amenities <- amenity_columns[grep("(pool|tennis|gym)", amenity_columns)]
top_amenities_with_additional <- unique(c(top_100_amenities, additional_amenities))
df_top_amenities_with_additional <- amenities_wide_df %>% dplyr::select(c("id", top_amenities_with_additional))
non_amenity_columns <- grep("^amenity_", colnames(amenities_wide_df), value = TRUE, invert = TRUE)
df_non_amenity <- amenities_wide_df %>%  dplyr::select(non_amenity_columns)
df_final_joined <- df_non_amenity %>% left_join(df_top_amenities_with_additional, by = "id")

# Remove outliers from the train_set dataframe using the IQR method
Q1 <- quantile(df_final_joined$price, 0.25)
Q3 <- quantile(df_final_joined$price, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 2.5 * IQR
upper_bound <- Q3 + 2.5 * IQR

# Plot the histogram before removing outliers
ggplot(df_final_joined, aes(x = price)) +
  geom_histogram(bins = 100) +
  xlim(0, 5000) +
  ylim(0,3000)

# Filter the data to remove the outliers
AirBnB_data <- df_final_joined[(df_final_joined$price >= lower_bound) & (df_final_joined$price <= upper_bound), ]

# Plot the histogram after removing outliers
ggplot(AirBnB_data, aes(x = price)) +
  geom_histogram(bins = 100) +
  xlim(0, 750)

# Set the filenames for the train and test data files
train_filename <- paste0(data_dir, "train_data.csv")
test_filename <- paste0(data_dir, "test_data.csv")

# Check if the train and test data files exist in the directory
if (!file.exists(train_filename) || !file.exists(test_filename)) {
  # If either file does not exist, split the data into train and test sets
  set.seed(123)
  split_index <- createDataPartition(AirBnB_data$id, p = 0.8, list = FALSE)
  train_data <- AirBnB_data[split_index, ]
  test_data <- AirBnB_data[-split_index, ]
  
  # Save the train and test sets to CSV files
  write.csv(train_data, paste0(data_dir,"train_data.csv"), row.names = FALSE)
  write.csv(test_data, paste0(data_dir,"test_data.csv"), row.names = FALSE)
} else {
  # If both files exist, read the train and test data sets from the files
  train_data <- read.csv(train_filename)
  test_data <- read.csv(test_filename)
}

# Print the dimensions of the train and test data sets
cat("Train data dimensions:", dim(train_data), "\n")
cat("Test data dimensions:", dim(test_data), "\n")

# Read the CSV file
train_data <- read.csv(paste0(data_dir, "train_data.csv"))
test_data <- read.csv(paste0(data_dir, "test_data.csv"))

# Define the linear regression model (replace "price" with your actual target variable if needed)
nullmodel <- lm(price ~ 1, data = train_data[, -1])
model <- lm(price ~ ., data = train_data[, -1])

# Define the stopping criterion to find the top N features
max_features <- 200  # Change this value to the desired number of features

# Set the filename for the RDS file
filename <- paste0(data_dir, "forward_result.rds")

# Check if the RDS file exists in the directory
if (file.exists(filename)) {
  
  # If the file exists, load the object from the file
  forward_result <- readRDS(filename)
} else {
  
  # If the file does not exist, run the stepAIC function to generate the forward_result object
  forward_result <- stepAIC(nullmodel, direction = "forward", trace = FALSE, scope = list(lower = nullmodel, upper = model), k = 2, steps = max_features)

  
  # Save the forward_result object as an RDS file
  saveRDS(forward_result, filename)
}

# Print a summaryr of the foward_result object
summary(forward_result)

# Get the names of all selected variables
selected_names <- names(coef(forward_result))[-1]  # exclude the intercept

# Sort the variables by their absolute coefficients and select the top 30
selected_names_sorted <- selected_names[order(abs(coef(forward_result)[-1]), decreasing = TRUE)]

# Rename variables with spaces in their names
selected_names_sorted <- gsub(" ", "_", selected_names_sorted)
selected_names_sorted <- gsub("`", "", selected_names_sorted)

# rename variables with spaces in their names
names(train_data)[grep(" ", names(train_data))] <- gsub(" ", "", names(train_data)[grep(" ", names(train_data))])

# create a new data frame with only the selected variables
train_data <- train_data[, c("price", "review_scores_checkin", "review_scores_communication", "review_scores_rating", "review_scores_accuracy", selected_names_sorted)]
test_data <- test_data[, c("price", "review_scores_checkin", "review_scores_communication", "review_scores_rating", "review_scores_accuracy",  selected_names_sorted)]

# create a new data frame with only the selected variables - price
train_set_no_price <- train_data[, c("review_scores_checkin", "review_scores_communication",  "review_scores_rating", "review_scores_accuracy",  selected_names_sorted)]
test_set_no_price <- test_data[, c("review_scores_checkin", "review_scores_communication", "review_scores_rating", "review_scores_accuracy", selected_names_sorted)]

#test labels
test_labels <- test_data$price
train_labels <- train_data$price
```

```{r}
#Predictive Models

#Linear Regression
model <- lm(price ~., data = train_data)
summary(model)

linear_preds <- predict(model, newdata = test_data)

#PCA
numeric_vars <- sapply(train_set_no_price, is.numeric)
pca_all_numeric <- prcomp(train_set_no_price[, numeric_vars], center = TRUE, scale. = TRUE)

summary(pca_all_numeric)
plot(pca_all_numeric)
biplot(pca_all_numeric, main = "PCA Biplot")

# PCA scree plot
pca_pve <- (pca_all_numeric$sdev^2) / sum(pca_all_numeric$sdev^2)
plot(pca_pve, type = "b", main = "PCA Scree Plot", xlab = "Principal Component", ylab = "Proportion of Variance Explained")
abline(v = which.max(diff(pca_pve)), col = "red", lty = 2) # Add a vertical line at the elbow point

#LASSO
x <- as.matrix(train_set_no_price)
y <- train_labels

alpha_lasso <- 1
lasso_model <- glmnet(x, y, alpha = alpha_lasso)

cv_lasso <- cv.glmnet(x, y, alpha = alpha_lasso)
best_lambda <- cv_lasso$lambda.min
lasso_model_best <- glmnet(x, y, alpha = alpha_lasso, lambda = best_lambda)

lasso_preds <- predict(lasso_model_best, as.matrix(test_set_no_price))


# LASSO Coefficients Plot
coef(lasso_model_best)

# Extract the coefficients of the Lasso model
beta <- coef(lasso_model)

# Convert the coefficient matrix to a data frame
beta_df <- as.data.frame(as.matrix(beta))
beta_df$predictor <- row.names(beta_df)

# Extract the coefficients of the Lasso model at the best lambda
beta_best <- coef(lasso_model_best)

# Convert the coefficient matrix to a data frame
beta_best_df <- as.data.frame(as.matrix(beta_best))
colnames(beta_best_df) <- c("coef")
beta_best_df$predictor <- row.names(beta_best_df)

# Compute the absolute value of the coefficients
beta_best_df$abs_coef <- abs(beta_best_df$coef)

# Sort the predictors by their absolute coefficients in descending order
beta_best_df_sorted <- beta_best_df[order(beta_best_df$abs_coef, decreasing = TRUE),]

# Select the top 15 predictors
top_15_predictors <- head(beta_best_df_sorted, 16) # We use 16 because the first row is the intercept
worst_15_predictors <- tail(beta_best_df_sorted, 16) # We use 16 because the first row is the intercept
top_15_predictors <- top_15_predictors[-1,] # Remove the intercept

# Convert the coefficient matrix from wide to long format
beta_long <- reshape::melt(beta_df, id = "predictor")
beta_long$variable <- as.numeric(gsub("s", "", beta_long$variable))

# Extract the lambda values and compute the L1 norm
beta_long$lambda <- lasso_model$lambda[beta_long$variable+1]
beta_long$norm <- apply(abs(beta[-1,]), 2, sum)[beta_long$variable+1]

# Filter beta_long to include only the top 15 predictors
beta_long_filtered <- beta_long %>%
  dplyr::filter(predictor %in% top_15_predictors$predictor)

# Create the ggplot with the legend
p <- ggplot(beta_long_filtered[beta_long_filtered$predictor != "(Intercept)",], aes(lambda, value, color = predictor)) + 
    geom_line() + 
    scale_x_log10() + 
    xlab("Lambda (log scale)") + 
    theme_bw()

# Convert the ggplot to a plotly object with tooltips
p_plotly <- ggplotly(p, tooltip = "text") %>%
    style(text = ~paste("Variable:", predictor), traces = c(1, 3, 5, 7, 9))

# Display the interactive plot
p_plotly

#Factor Analysis
scaled_data <- scale(train_set_no_price[, numeric_vars])
n_factors <- 2
fa_result <- factanal(scaled_data, factors = n_factors, rotation = "varimax")

print(fa_result$loadings)
fa_scores <- fa_result$scores

# Factor Analysis Loadings Plot
loadings_df <- as.data.frame.matrix(fa_result$loadings)
loadings_df$variable <- rownames(fa_result$loadings)
loadings_long <- tidyr::gather(loadings_df, factor, loading, -variable)

ggplot(loadings_long, aes(x = factor, y = variable, fill = loading, size = abs(loading))) +
  geom_point(shape = 21) +
  scale_size_continuous(range = c(1, 8)) +
  theme_minimal() +
  labs(title = "Factor Loadings Plot")

# Factor analysis scree plot
fa_eigenvalues <- fa_result$uniquenesses / sum(fa_result$uniquenesses)
plot(fa_eigenvalues, type = "b", main = "Factor Analysis Scree Plot", xlab = "Factor", ylab = "Proportion of Variance Explained")
abline(v = which.max(diff(fa_eigenvalues)), col = "red", lty = 2) # Add a vertical line at the elbow point

# Performance Metrics
linear_regression_mse <- mean((test_labels - linear_preds)^2)
lasso_mse <- mean((test_labels - lasso_preds)^2)
linear_regression_mae <- mean(abs(test_labels - linear_preds))
lasso_mae <- mean(abs(test_labels - lasso_preds))

cat("Linear Regression MSE:", linear_regression_mse, "MAE:", linear_regression_mae, "\n")
cat("LASSO Regression MSE:", lasso_mse, "MAE:", lasso_mae, "\n")

# Evaluation summary
evaluation <- data.frame(Model = c("Linear Regression", "LASSO"),
                         MSE = c(linear_regression_mse, lasso_mse),
                         MAE = c(linear_regression_mae, lasso_mae))
print(evaluation)
```
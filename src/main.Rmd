---
title: "main"
output: html_document
date: "2023-04-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Data
```{r}
#data_dir <- "../data/"
#listing <- read.csv(paste0(data_dir,"listings.csv"))
#listing2 <- read.csv(paste0(data_dir, "listings 2.csv"))
#neighbourhoods <- read.csv(paste0(data_dir, "neighbourhoods.csv"))
#reviews <- read.csv(paste0(data_dir, "reviews.csv"))
#reviews2 <- read.csv(paste0(data_dir, "reviews 2.csv"))

listing <- read.csv(paste0("/Users/loganheft/Desktop/data-2/listings.csv"))
listing2 <- read.csv(paste0("/Users/loganheft/Desktop/data-2/listings 2.csv"))
neighbourhoods <- read.csv(paste0("/Users/loganheft/Desktop/data-2/neighbourhoods.csv"))
reviews <- read.csv(paste0("/Users/loganheft/Desktop/data-2/reviews.csv"))
reviews2 <- read.csv(paste0("/Users/loganheft/Desktop/data-2/reviews 2.csv"))

```
Our data covers the quarterly data from last year.

# Data Preparation
``` {r}
# Select relevant features
listing_col <- c("id", "neighbourhood_group", "neighbourhood", "latitude", "longitude", "room_type", "price")
listing2_col <- c("id", "property_type", "accommodates", "bedrooms", "beds", "amenities", "review_scores_rating",
                  "review_scores_accuracy", "review_scores_cleanliness", "review_scores_checkin", "review_scores_communication", 
                  "review_scores_location", "review_scores_value", "instant_bookable")
listing_sub <- subset(listing, select=listing_col)
listing2_sub <- subset(listing2,select=listing2_col)

# Merge listing and listing2
data <- merge(listing_sub, listing2_sub, by = "id", all.x = TRUE)

# Check for missing values
print("Missing values:")
colSums(is.na(data))
```
There some missing values, especially in the ratings variables. We will use MICE imputation to impute the missing values.
``` {r}
library(mice)

data <- data %>%
  filter(price < quantile(price, 0.99))

# Select variables with missing values
vars <- c("bedrooms", "beds", "review_scores_rating", "review_scores_accuracy", 
          "review_scores_cleanliness", "review_scores_checkin", "review_scores_communication", 
          "review_scores_location", "review_scores_value")

# Perform MICE imputation
imp <- mice(data[, vars], method = "pmm", m = 5, maxit = 50, seed = 12)


# Replace the missing values in the original dataset with the imputed values
data[, vars] <- complete(imp)

# Check for any remaining missing values
sapply(data, function(x) sum(is.na(x)))

```

#EDA
``` {r}
library(tidyverse)
library(lubridate)
library(ggplot2)

data <- data %>%
  filter(price < quantile(price, 0.99))

# Summarize the numerical variables
summary(data$price)
summary(data$review_scores_rating)

# Plot the distribution of the price variable
ggplot(data, aes(price)) + 
  geom_histogram() + 
  xlim(0, 3000)

ggplot(data, aes(price)) + 
  geom_histogram() + 
  xlim(0, 500)

# Plot the relationship between price and number of bedrooms
ggplot(data, aes(x = bedrooms, y = price)) + 
  geom_point()

# Show the average price for the top 20 most popular property types and sort the plot
top_property_types <- data %>% 
  group_by(property_type) %>% 
  summarize(num_properties = n()) %>% 
  top_n(20, num_properties)

data %>% 
  filter(property_type %in% top_property_types$property_type) %>% 
  group_by(property_type) %>% 
  summarize(avg_price = mean(price)) %>% 
  arrange(desc(avg_price)) %>% # Sort by average price in descending order
  ggplot(aes(x = property_type, y = avg_price)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Property Type", y = "Average Price")

# Get the top 20 neighborhoods by average price
top_neighborhoods <- data %>% 
  group_by(neighbourhood) %>% 
  summarize(avg_price = mean(price)) %>% 
  top_n(20, avg_price)

data %>% 
  filter(neighbourhood %in% top_neighborhoods$neighbourhood) %>% 
  group_by(neighbourhood) %>% 
  summarize(avg_price = mean(price)) %>% 
  ggplot(aes(x = reorder(neighbourhood, -avg_price), y = avg_price)) +
  geom_col() +
  coord_flip() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8)) +
  theme(plot.margin = unit(c(1, 1, 1, 1), "cm")) +
  labs(x = "Neighborhood", y = "Average Price")



# Create a new variable for the month of the year
data$month <- month(ymd(data$last_review))

# Plot the number of reviews by month
data %>% 
  group_by(month) %>% 
  summarize(num_reviews = n()) %>% 
  ggplot(aes(x = month, y = num_reviews)) +
  geom_line() +
  scale_x_continuous(breaks = 1:12, labels = month.abb)


<<<<<<< Updated upstream

=======
# Remove original categorical columns
df <- df[, !(colnames(df) %in% cat_columns)]

# Clean amenities and create binary columns for each amenity
df <- df %>%
  mutate(amenities = str_remove_all(amenities, "\\[|\\]|\"")) %>%
  separate_rows(amenities, sep = ", ") %>%
  mutate(amenity_clean = clean_colnames(amenities)) %>%
  mutate(amenities = paste0("amenity_", amenity_clean),
         amenities = ifelse(substr(amenities, 1, 10) == "amenity__", substr(amenities, 10), amenities),
         value = 1) %>%
  distinct(id, amenities, .keep_all = TRUE)

# Remove the 'amenity_clean' column
df <- dplyr::select(df, -amenity_clean)

## Keep only the top 100 amenities and additional amenities
amenities_wide_df <- df %>% pivot_wider(names_from = amenities, values_from = value, values_fill = 0)
amenity_columns <- grep("^amenity_", colnames(amenities_wide_df), value = TRUE)
amenity_sums <- colSums(amenities_wide_df[amenity_columns], na.rm = TRUE)
sorted_amenities <- sort(amenity_sums, decreasing = TRUE)
top_100_amenities <- names(sorted_amenities)[1:100]
additional_amenities <- amenity_columns[grep("(pool|tennis|gym)", amenity_columns)]
top_amenities_with_additional <- unique(c(top_100_amenities, additional_amenities))
df_top_amenities_with_additional <- amenities_wide_df %>% dplyr::select(c("id", top_amenities_with_additional))
non_amenity_columns <- grep("^amenity_", colnames(amenities_wide_df), value = TRUE, invert = TRUE)
df_non_amenity <- amenities_wide_df %>%  dplyr::select(non_amenity_columns)
df_final_joined <- df_non_amenity %>% left_join(df_top_amenities_with_additional, by = "id")

# Remove outliers from the train_set dataframe using the IQR method
Q1 <- quantile(df_final_joined$price, 0.25)
Q3 <- quantile(df_final_joined$price, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 2.5 * IQR
upper_bound <- Q3 + 2.5 * IQR

# Plot the histogram before removing outliers
ggplot(df_final_joined, aes(x = price)) +
  geom_histogram(bins = 100) +
  xlim(0, 5000) +
  ylim(0,3000)

# Filter the data to remove the outliers
AirBnB_data <- df_final_joined[(df_final_joined$price >= lower_bound) & (df_final_joined$price <= upper_bound), ]

# Plot the histogram after removing outliers
ggplot(AirBnB_data, aes(x = price)) +
  geom_histogram(bins = 100) +
  xlim(0, 750)

# Set the filenames for the train and test data files
train_filename <- paste0(data_dir, "train_data.csv")
test_filename <- paste0(data_dir, "test_data.csv")

# Check if the train and test data files exist in the directory
if (!file.exists(train_filename) || !file.exists(test_filename)) {
  # If either file does not exist, split the data into train and test sets
  set.seed(123)
  split_index <- createDataPartition(AirBnB_data$id, p = 0.8, list = FALSE)
  train_data <- AirBnB_data[split_index, ]
  test_data <- AirBnB_data[-split_index, ]
  
  # Save the train and test sets to CSV files
  write.csv(train_data, paste0(data_dir,"train_data.csv"), row.names = FALSE)
  write.csv(test_data, paste0(data_dir,"test_data.csv"), row.names = FALSE)
} else {
  # If both files exist, read the train and test data sets from the files
  train_data <- read.csv(train_filename)
  test_data <- read.csv(test_filename)
}

# Print the dimensions of the train and test data sets
cat("Train data dimensions:", dim(train_data), "\n")
cat("Test data dimensions:", dim(test_data), "\n")
>>>>>>> Stashed changes
```

```{r}
# Load the necessary libraries and read in the data
library(dplyr)
library(psych)
library(corrplot)
data <- read.csv("path/to/airbnb/dataset.csv")

# Remove outliers
data <- data %>%
  filter(price < quantile(price, 0.99))

<<<<<<< Updated upstream
=======
# Define the stopping criterion to find the top N features
max_features <- 200  # Change this value to the desired number of features

forward_result <- stepAIC(
nullmodel,
direction = "forward",
trace = FALSE,
scope = list(lower = nullmodel, upper = model),
k = 2,
steps = max_features)

  
# Set the filename for the RDS file
filename <- paste0(data_dir, "forward_result.rds")

# Check if the RDS file exists in the directory
if (file.exists(filename)) {
  # If the file exists, load the object from the file
  forward_result <- readRDS(filename)
} else {
  # If the file does not exist, run the stepAIC function to generate the forward_result object
  forward_result <- stepAIC(
  nullmodel,
  direction = "forward",
  trace = FALSE,
  scope = list(lower = nullmodel, upper = model),
  k = 2,
  steps = max_features)
  
  # Save the forward_result object as an RDS file
  saveRDS(forward_result, filename)
}

# Print a summary of the forward_result object
summary(forward_result)

selected_names <- names(coef(forward_result))[-1]  # exclude the intercept
selected_names <- gsub(" ", "_", selected_names)
selected_names <- gsub("`", "", selected_names)

# rename variables with spaces in their names
names(train_data)[grep(" ", names(train_data))] <- gsub(" ", "", names(train_data)[grep(" ", names(train_data))])

# create a new data frame with only the selected variables
train_data <- train_data[, c("price", "review_scores_checkin", "review_scores_communication", "review_scores_rating", "review_scores_accuracy", selected_names)]
test_data <- test_data[, c("price", "review_scores_checkin", "review_scores_communication", "review_scores_rating", "review_scores_accuracy", selected_names)]

# create a new data frame with only the selected variables - price
train_set_no_price <- train_data[, c("review_scores_checkin", "review_scores_communication", "review_scores_rating", "review_scores_accuracy", selected_names)]
test_set_no_price <- test_data[, c("review_scores_checkin", "review_scores_communication", "review_scores_rating", "review_scores_accuracy", selected_names)]

#test labels
test_labels <- test_data$price
train_labels <- train_data$price
```

#Predictive Models
```{r}
#checks for linearity




#Linear Regression
>>>>>>> Stashed changes
# Perform a linear regression to predict price
#If somebody wants to do some preprocessing of the data then we can use more factors
#I only wrote the code for the initial leg work


model <- lm(price ~ accommodates + bedrooms + review_scores_rating, data = data)
summary(model)

data <- na.omit(data)

# Perform a principal component analysis
pca <- prcomp(data[,c("accommodates", "bedrooms", "review_scores_rating")], center = TRUE, scale. = TRUE)

summary(pca)

plot(pca)

#principal components
pca$rotation

# principal component scores for each observation
pca$x

fa_result <- factanal(data[,c("accommodates", "bedrooms", "review_scores_rating")], factors = 1, rotation = "varimax")

#factor loadings
print(fa_result$loadings)

```


```{r}

```

```{r}

```
